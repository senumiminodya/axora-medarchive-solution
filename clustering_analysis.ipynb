{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d74523d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Imports ready.\n"
     ]
    }
   ],
   "source": [
    "# ===== Notebook 3: Clustering Analysis =====\n",
    "# Axora: Clinical Document Classification\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from datasets import load_dataset\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import (\n",
    "    homogeneity_score, completeness_score, v_measure_score,\n",
    "    adjusted_rand_score, adjusted_mutual_info_score, silhouette_score\n",
    ")\n",
    "\n",
    "from collections import Counter\n",
    "import textwrap\n",
    "import warnings, os, sys, re, math, random\n",
    "\n",
    "# Plot aesthetics\n",
    "sns.set(style=\"whitegrid\", font_scale=1.05)\n",
    "plt.rcParams[\"figure.figsize\"] = (10, 6)\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "random.seed(RANDOM_STATE)\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "print(\"✅ Imports ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bf99cf31",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Dataset Overview (Notebook 3) ===\n",
      "Total documents: 2464\n",
      "  Train: 1724 | Val: 370 | Test: 370\n",
      "Unique specialties: 13\n",
      "medical_specialty\n",
      "Cardiovascular / Pulmonary    742\n",
      "Orthopedic                    408\n",
      "Neurology                     282\n",
      "Gastroenterology              222\n",
      "Obstetrics / Gynecology       182\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Load the same dataset used in Notebook 1\n",
    "dataset = load_dataset(\"hpe-ai/medical-cases-classification-tutorial\")\n",
    "\n",
    "train_df = pd.DataFrame(dataset[\"train\"])\n",
    "val_df   = pd.DataFrame(dataset[\"validation\"])\n",
    "test_df  = pd.DataFrame(dataset[\"test\"])\n",
    "\n",
    "# Keep only the fields we need\n",
    "cols = [\"transcription\", \"medical_specialty\"]\n",
    "train_df = train_df[cols].copy()\n",
    "val_df   = val_df[cols].copy()\n",
    "test_df  = test_df[cols].copy()\n",
    "\n",
    "# Combine for unsupervised clustering (we'll still keep true labels for evaluation)\n",
    "all_df = pd.concat(\n",
    "    [\n",
    "        train_df.assign(split=\"train\"),\n",
    "        val_df.assign(split=\"val\"),\n",
    "        test_df.assign(split=\"test\"),\n",
    "    ],\n",
    "    axis=0,\n",
    "    ignore_index=True\n",
    ")\n",
    "\n",
    "# Basic overview\n",
    "print(\"=== Dataset Overview (Notebook 3) ===\")\n",
    "print(f\"Total documents: {len(all_df)}\")\n",
    "print(f\"  Train: {len(train_df)} | Val: {len(val_df)} | Test: {len(test_df)}\")\n",
    "print(f\"Unique specialties: {all_df['medical_specialty'].nunique()}\")\n",
    "print(all_df[\"medical_specialty\"].value_counts().head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0fc0491d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample cleaned text:\n",
      " procedure note pacemaker icd interrogation history of present illness the patient is a 67 year old gentleman who was admitted to the hospital he has had icd pacemaker implantation this is a st [...]\n"
     ]
    }
   ],
   "source": [
    "# Minimal cleaner for robustness (keep consistent with earlier logic)\n",
    "def clean_text(text: str) -> str:\n",
    "    if not isinstance(text, str):\n",
    "        text = str(text)\n",
    "    text = text.lower()\n",
    "    # keep letters, digits, and spaces; remove boilerplate symbols\n",
    "    text = re.sub(r\"[^a-z0-9\\s]\", \" \", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    return text\n",
    "\n",
    "all_df[\"text_clean\"] = all_df[\"transcription\"].map(clean_text)\n",
    "print(\"Sample cleaned text:\\n\", textwrap.shorten(all_df.iloc[0][\"text_clean\"], width=200))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8e3108ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== TF-IDF Matrix ===\n",
      "Shape: (2464, 5000)\n",
      "Features (first 10): ['00' '000' '000 epinephrine' '01' '02' '03' '04' '05' '06' '07']\n"
     ]
    }
   ],
   "source": [
    "# Match Notebook 1 settings: max_features=5000, english stopwords, ngram_range=(1,2)\n",
    "vectorizer = TfidfVectorizer(\n",
    "    max_features=5000,\n",
    "    stop_words=\"english\",\n",
    "    ngram_range=(1, 2),\n",
    "    lowercase=False,   # we already lowercased\n",
    "    norm=\"l2\"          # default; good for cosine-like behavior with euclidean\n",
    ")\n",
    "\n",
    "X_all = vectorizer.fit_transform(all_df[\"text_clean\"])\n",
    "feature_names = np.array(vectorizer.get_feature_names_out())\n",
    "\n",
    "print(\"=== TF-IDF Matrix ===\")\n",
    "print(\"Shape:\", X_all.shape)\n",
    "print(\"Features (first 10):\", feature_names[:10])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
